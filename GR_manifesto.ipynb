{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFJxMcYSlxki/gfXiCFW1O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ethanrom/NLP-stuff/blob/main/GR_manifesto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "extractive text summarization"
      ],
      "metadata": {
        "id": "whUpFHV1wfba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "!pip install -q wordcloud\n",
        "import wordcloud\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger') \n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import unicodedata\n",
        "import numpy as np\n",
        "import re\n",
        "import string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aqtNKYgxeuo",
        "outputId": "e82215f3-68f7-4e58-ef35-75af37f39d53"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VwGgvb6AwbIC"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "def summarize(text, ratio=0.5):\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    words = word_tokenize(text)\n",
        "    words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]\n",
        "\n",
        "    fdist = FreqDist(words)\n",
        "    top_words = [word[0] for word in fdist.most_common(int(len(words) * ratio))]\n",
        "\n",
        "    sentences = sent_tokenize(text)\n",
        "    summary = []\n",
        "    for sentence in sentences:\n",
        "        for word in top_words:\n",
        "            if word in sentence.lower():\n",
        "                summary.append(sentence)\n",
        "                break\n",
        "\n",
        "    return \" \".join(summary)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    with open(\"a.txt\", \"r\") as f:\n",
        "        text = f.read()\n",
        "    summary = summarize(text)\n",
        "\n",
        "    with open(\"output.txt\", \"w\") as f:\n",
        "        f.write(summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "counting debt mentions"
      ],
      "metadata": {
        "id": "_fwIWfRnyfOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_debt(text):\n",
        "    words = text.split()\n",
        "    debt_count = words.count(\"debt\")\n",
        "    total_count = len(words)\n",
        "    return (debt_count / total_count) * 100\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    with open(\"a.txt\", \"r\") as f:\n",
        "        text = f.read()\n",
        "    debt_percentage = count_debt(text)\n",
        "    print(\"The word 'debt' appears in the text {:.2f}% of the time\".format(debt_percentage))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxHn2SdlyeG4",
        "outputId": "54d5fecc-70aa-4372-bf11-d907a9030875"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The word 'debt' appears in the text 0.03% of the time\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Latent Dirichlet Allocation"
      ],
      "metadata": {
        "id": "ffMwFpAozMTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from gensim import corpora\n",
        "\n",
        "def preprocess(text):\n",
        "    return [word for word in simple_preprocess(text) if word not in STOPWORDS]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    with open(\"a.txt\", \"r\") as f:\n",
        "        text = f.read()\n",
        "    documents = text.splitlines()\n",
        "    processed_docs = [preprocess(doc) for doc in documents]\n",
        "    dictionary = corpora.Dictionary(processed_docs)\n",
        "    bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "    lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=5, id2word=dictionary, passes=2, workers=2)\n",
        "    for idx, topic in lda_model.print_topics(-1):\n",
        "        print(\"Topic: {} \\nWords: {}\".format(idx, topic))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J25Gcgx5zNRD",
        "outputId": "561067a5-1766-4d7f-c677-4673bc2d77d8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic: 0 \n",
            "Words: 0.009*\"country\" + 0.008*\"taken\" + 0.007*\"provided\" + 0.006*\"government\" + 0.006*\"sector\" + 0.005*\"introduced\" + 0.005*\"people\" + 0.004*\"steps\" + 0.004*\"programme\" + 0.004*\"training\"\n",
            "Topic: 1 \n",
            "Words: 0.010*\"national\" + 0.007*\"facilities\" + 0.006*\"ensure\" + 0.006*\"country\" + 0.006*\"economic\" + 0.005*\"state\" + 0.005*\"people\" + 0.005*\"security\" + 0.005*\"programme\" + 0.005*\"development\"\n",
            "Topic: 2 \n",
            "Words: 0.011*\"government\" + 0.007*\"people\" + 0.007*\"national\" + 0.007*\"public\" + 0.006*\"country\" + 0.006*\"services\" + 0.005*\"billion\" + 0.005*\"develop\" + 0.005*\"transport\" + 0.004*\"agricultural\"\n",
            "Topic: 3 \n",
            "Words: 0.011*\"country\" + 0.011*\"new\" + 0.008*\"education\" + 0.007*\"policy\" + 0.006*\"ensure\" + 0.006*\"facilities\" + 0.006*\"public\" + 0.005*\"industry\" + 0.005*\"national\" + 0.005*\"development\"\n",
            "Topic: 4 \n",
            "Words: 0.014*\"development\" + 0.009*\"country\" + 0.009*\"sri\" + 0.009*\"people\" + 0.007*\"lanka\" + 0.006*\"social\" + 0.005*\"tax\" + 0.004*\"programme\" + 0.004*\"international\" + 0.004*\"economic\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "extract claim  using Named Entity Recognition"
      ],
      "metadata": {
        "id": "ant2kDR11ccU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRznOA_v0w8r",
        "outputId": "ceca60eb-b2b2-4ac3-9370-18cf6fef512b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2023-02-06 17:43:24.415259: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-sm==3.4.1) (3.4.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.12)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (6.3.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.25.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.0.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#spaCy model lable CLAIM\n",
        "!gdown 1Y6A3cpohnPuspB0umy8FEqXHs_2TBHdQ"
      ],
      "metadata": {
        "id": "bT_JT_N41s7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "def extract_claims(text):\n",
        "    # Load the spaCy model\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    \n",
        "    # Parse the text\n",
        "    doc = nlp(text)\n",
        "    \n",
        "    # Iterate over the entities in the document\n",
        "    claims = []\n",
        "    for ent in doc.ents:\n",
        "        # If the entity is a claim, add it to the list of claims\n",
        "        if ent.label_ == \"CLAIM\":\n",
        "            claims.append(ent.text)\n",
        "    \n",
        "    return claims\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    with open(\"a.txt\", \"r\") as f:\n",
        "        text = f.read()\n",
        "    claims = extract_claims(text)\n",
        "    for claim in claims:\n",
        "        print(claim)"
      ],
      "metadata": {
        "id": "VoS-PM-d0W0-"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}